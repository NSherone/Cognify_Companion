{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Research Papers from arXiv API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to arxiv_papers_with_pdf.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# Define the arXiv API base URL and query parameters\n",
    "arxiv_base_url = \"http://export.arxiv.org/api/query?\"\n",
    "search_query = 'cat:cs.*'  \n",
    "max_results = 400  \n",
    "start = 0\n",
    "\n",
    "# Prepare an empty list to store the paper data\n",
    "papers_data = []\n",
    "\n",
    "# Function to fetch data from arXiv API\n",
    "def fetch_arxiv_data(start, max_results):\n",
    "    url = f\"{arxiv_base_url}search_query={search_query}&start={start}&max_results={max_results}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Error fetching data: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to parse the arXiv response and extract metadata\n",
    "def parse_arxiv_response(response_text):\n",
    "    root = ET.fromstring(response_text)\n",
    "    namespaces = {'': 'http://www.w3.org/2005/Atom'}  # XML namespace for arXiv response\n",
    "    papers = []\n",
    "    \n",
    "    for entry in root.findall('entry', namespaces):\n",
    "        pdf_url = None\n",
    "        # Extract PDF URL from the links\n",
    "        for link in entry.findall('link', namespaces):\n",
    "            if 'pdf' in link.get('title', '').lower():\n",
    "                pdf_url = link.get('href')\n",
    "                break\n",
    "        \n",
    "        paper = {\n",
    "            'title': entry.find('title', namespaces).text,\n",
    "            'id': entry.find('id', namespaces).text,\n",
    "            'published': entry.find('published', namespaces).text,\n",
    "            'summary': entry.find('summary', namespaces).text,\n",
    "            'authors': ', '.join([author.find('name', namespaces).text for author in entry.findall('author', namespaces)]),\n",
    "            'categories': entry.find('category', namespaces).get('term'),\n",
    "            'pdf_url': pdf_url  # Add the PDF URL to the data\n",
    "        }\n",
    "        papers.append(paper)\n",
    "    \n",
    "    return papers\n",
    "\n",
    "# Fetch and process the data\n",
    "response_text = fetch_arxiv_data(start, max_results)\n",
    "if response_text:\n",
    "    papers = parse_arxiv_response(response_text)\n",
    "    papers_df = pd.DataFrame(papers)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    papers_df.to_csv(\"arxiv_papers_with_pdf.csv\", index=False)\n",
    "    print(\"Data saved to arxiv_papers_with_pdf.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/400 [00:00<01:37,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to download http://arxiv.org/pdf/cs/0701021v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 61/400 [00:06<00:45,  7.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to download http://arxiv.org/pdf/1012.4170v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 91/400 [00:12<01:59,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to download http://arxiv.org/pdf/0911.2829v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 102/400 [00:35<21:37,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to download http://arxiv.org/pdf/1009.3306v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 139/400 [00:44<01:33,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to download http://arxiv.org/pdf/1108.3558v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 142/400 [01:05<12:31,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to download http://arxiv.org/pdf/1202.4535v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 145/400 [01:10<09:01,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: could not parse color space (142 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (142 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (237 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (237 0 R)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 167/400 [02:02<09:04,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to download http://arxiv.org/pdf/1904.06159v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 212/400 [05:04<12:40,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to download http://arxiv.org/pdf/2112.14770v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 218/400 [05:42<12:47,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to download http://arxiv.org/pdf/1001.4573v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 219/400 [05:45<11:49,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to download http://arxiv.org/pdf/2211.10675v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 222/400 [05:53<09:52,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to download http://arxiv.org/pdf/2404.13672v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 266/400 [07:22<04:58,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to download http://arxiv.org/pdf/2206.01250v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 301/400 [08:28<02:33,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 315/400 [09:51<19:57, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to download http://arxiv.org/pdf/1405.2281v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 320/400 [09:58<04:49,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: could not parse color space (327 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (370 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (456 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (479 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (550 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (674 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (692 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (727 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (769 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (802 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (840 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (858 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (898 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (914 0 R)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 355/400 [11:14<01:17,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to download http://arxiv.org/pdf/1511.02528v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 370/400 [12:01<01:35,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to download http://arxiv.org/pdf/2309.07166v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [13:18<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JSON file saved as arxiv_papers_with_full_text.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to download a PDF file\n",
    "def download_pdf(pdf_url, save_dir=\"pdfs\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Create directory if not exists\n",
    "    pdf_filename = os.path.join(save_dir, pdf_url.split('/')[-1])  # Extract filename from URL\n",
    "    \n",
    "    if not os.path.exists(pdf_filename):  # Skip if already downloaded\n",
    "        response = requests.get(pdf_url)\n",
    "        if response.status_code == 200:\n",
    "            with open(pdf_filename, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            return pdf_filename\n",
    "        else:\n",
    "            print(f\"❌ Failed to download {pdf_url}\")\n",
    "            return None\n",
    "    return pdf_filename\n",
    "\n",
    "# Function to extract full text from a PDF\n",
    "def extract_full_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)  # Open the PDF\n",
    "    return \"\\n\".join(page.get_text(\"text\") for page in doc)  # Merge all pages\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = \"arxiv_papers_with_pdf.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Create a list to store extracted data\n",
    "papers_data = []\n",
    "\n",
    "# Extract text for each PDF and store in JSON\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    paper_info = {\n",
    "        \"title\": row[\"title\"],\n",
    "        \"id\": row[\"id\"],\n",
    "        \"pdf_url\": row[\"pdf_url\"],\n",
    "        \"full_text\": \"\"  # Default empty if extraction fails\n",
    "    }\n",
    "\n",
    "    pdf_path = download_pdf(row[\"pdf_url\"])  # Download the PDF\n",
    "    if pdf_path:\n",
    "        paper_info[\"full_text\"] = extract_full_text(pdf_path)  \n",
    "\n",
    "    papers_data.append(paper_info)  # Append to list\n",
    "\n",
    "# Save as JSON\n",
    "json_file = \"arxiv_papers_with_full_text.json\"\n",
    "with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(papers_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ JSON file saved as {json_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
